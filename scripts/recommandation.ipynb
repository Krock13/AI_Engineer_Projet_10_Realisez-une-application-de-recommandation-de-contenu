{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook illustre comment :\n",
    "1. Charger et préparer les données (articles + interactions).\n",
    "2. Implémenter deux systèmes de recommandation : \n",
    "     - Collaborative Filtering (user-based kNN) \n",
    "     - Content-Based Filtering (similarité embeddings).\n",
    "3. Comparer sommairement les performances (hit rate).\n",
    "4. Sauvegarder les modèles et données transformées au format pickle.\n",
    "5. Fournir des fonctions réutilisables pour la prédiction \n",
    "   dans un environnement serverless (Azure Functions) ou Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# scipy.sparse pour la matrice sparse\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "# Pour désactiver certains warnings éventuels\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths vers les données\n",
    "PATH_ARTICLES_EMBEDDINGS = \"../data/articles_embeddings.pickle\"\n",
    "PATH_ARTICLES_METADATA   = \"../data/articles_metadata.csv\"\n",
    "PATH_CLICKS_PATTERN      = \"../data/clicks/clicks_hour_*.csv\"  # pattern pour charger les 385 fichiers\n",
    "\n",
    "# Paths pour sauvegarde des modèles / artefacts\n",
    "PATH_SAVE_CF_MODEL       = \"model_cf.pkl\"\n",
    "PATH_SAVE_CB_MODEL       = \"model_cb.pkl\"\n",
    "PATH_SAVE_DATA_TRANSFORM = \"data_transform.pkl\"  # pour tout ce qui est nécessaire (matrices, etc.)\n",
    "\n",
    "# Nombre de recommandations à retourner\n",
    "TOP_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_click_data(clicks_path_pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Charge les fichiers de clics (user interactions) stockés dans plusieurs CSV \n",
    "    et les concatène en un unique DataFrame.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - clicks_path_pattern : str\n",
    "        Chemin ou pattern glob pour trouver les fichiers CSV des clics.\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - pd.DataFrame\n",
    "        DataFrame contenant toutes les interactions (tous les clics).\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(clicks_path_pattern)\n",
    "    df_list = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "    df_clicks = pd.concat(df_list, ignore_index=True)\n",
    "    return df_clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_data(embeddings_path: str, metadata_path: str) -> tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Charge la matrice d'embeddings des articles ainsi que leurs métadonnées.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - embeddings_path : str\n",
    "        Chemin vers le fichier pickle contenant un numpy.ndarray des embeddings.\n",
    "    - metadata_path : str\n",
    "        Chemin vers le fichier CSV contenant les métadonnées (article_id, category_id, etc.).\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - (np.ndarray, pd.DataFrame)\n",
    "        - Matrice d'embeddings (shape (364047, 250))\n",
    "        - DataFrame des métadonnées (364047 lignes, 5 colonnes)\n",
    "    \"\"\"\n",
    "    # Chargement des embeddings\n",
    "    with open(embeddings_path, 'rb') as f:\n",
    "        articles_embeddings = pickle.load(f)  # type: np.ndarray\n",
    "    # Chargement du CSV de métadonnées\n",
    "    articles_metadata = pd.read_csv(metadata_path)\n",
    "    return articles_embeddings, articles_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_click_data(df_clicks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Exécute un prétraitement minimal sur les données de clics :\n",
    "    - Conversion de certaines colonnes (si nécessaire)\n",
    "    - Tri / suppression de colonnes inutiles (facultatif)\n",
    "    - Filtrage éventuel d'anomalies\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - df_clicks : pd.DataFrame\n",
    "        DataFrame des clics déjà concaténés.\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - pd.DataFrame\n",
    "        DataFrame pré-traité.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dans cet exemple, on va simplement s'assurer qu'on a bien user_id et click_article_id\n",
    "    df_clicks = df_clicks.dropna(subset=['user_id', 'click_article_id'])  # enlever lignes incomplètes\n",
    "    df_clicks['user_id'] = df_clicks['user_id'].astype(int)\n",
    "    df_clicks['click_article_id'] = df_clicks['click_article_id'].astype(int)\n",
    "\n",
    "    return df_clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Taille du DataFrame de clics : (2988181, 12)\n",
      "Taille de la matrice d'embeddings : (364047, 250)\n",
      "Taille du DataFrame de métadonnées : (364047, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement des données...\")\n",
    "\n",
    "# Chargement des embeddings et métadonnées\n",
    "articles_embeddings, articles_metadata = load_articles_data(PATH_ARTICLES_EMBEDDINGS, PATH_ARTICLES_METADATA)\n",
    "\n",
    "# Chargement des logs de clic\n",
    "df_clicks_raw = load_click_data(PATH_CLICKS_PATTERN)\n",
    "\n",
    "# Prétraitement\n",
    "df_clicks = preprocess_click_data(df_clicks_raw)\n",
    "\n",
    "print(\"Taille du DataFrame de clics :\", df_clicks.shape)\n",
    "print(\"Taille de la matrice d'embeddings :\", articles_embeddings.shape)\n",
    "print(\"Taille du DataFrame de métadonnées :\", articles_metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles distincts dans df_clicks : 46033\n",
      "Aucun article_id hors de la plage [0..364047).\n"
     ]
    }
   ],
   "source": [
    "# Combien d'articles distincts dans df_clicks ?\n",
    "unique_articles_in_clicks = df_clicks['click_article_id'].unique()\n",
    "print(\"Nombre d'articles distincts dans df_clicks :\", len(unique_articles_in_clicks))\n",
    "\n",
    "# Vérifier si certains IDs sont hors [0..364047)\n",
    "out_of_range_ids = unique_articles_in_clicks[\n",
    "    (unique_articles_in_clicks < 0) | (unique_articles_in_clicks >= articles_embeddings.shape[0])\n",
    "]\n",
    "if len(out_of_range_ids) > 0:\n",
    "    print(f\"ATTENTION: {len(out_of_range_ids)} article_ids sont hors de la plage [0..364047).\")\n",
    "    print(\"Exemple d'IDs hors range :\", out_of_range_ids[:20])\n",
    "else:\n",
    "    print(\"Aucun article_id hors de la plage [0..364047).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un DataFrame user-article, avec le nombre de clics par (user, article).\n",
    "Dans le cas d'un CF basé sur les interactions implicites, la \"note\" peut être le nombre de clics (ou 1 s'il y a au moins un clic).\n",
    "Ici, nous allons simplifier et compter le nombre total de clics user->article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction de la matrice (sparse) user-article...\n",
      "Taille df_user_item : (2950710, 3)\n",
      "Nombre d'utilisateurs (df_user_item) : 322897\n",
      "Nombre d'articles (df_user_item) : 46033\n",
      "Taille de la matrice sparse (CSR) : (322897, 46033)\n",
      "Nb d'articles présents dans df_clicks mais pas dans la matrice pivot: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Construction de la matrice (sparse) user-article...\")\n",
    "\n",
    "# GroupBy pour obtenir (user_id, article_id, click_count)\n",
    "df_user_item = (\n",
    "    df_clicks\n",
    "    .groupby(['user_id', 'click_article_id'])['click_timestamp']\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={'click_timestamp': 'click_count'})\n",
    ")\n",
    "\n",
    "print(\"Taille df_user_item :\", df_user_item.shape)\n",
    "print(\"Nombre d'utilisateurs (df_user_item) :\", df_user_item['user_id'].nunique())\n",
    "print(\"Nombre d'articles (df_user_item) :\", df_user_item['click_article_id'].nunique())\n",
    "\n",
    "# Liste unique des user_id et article_id\n",
    "user_ids = df_user_item[\"user_id\"].unique()\n",
    "article_ids = df_user_item[\"click_article_id\"].unique()\n",
    "\n",
    "# On crée des maps pour passer de l'ID réel à un index consécutif\n",
    "user_id_to_index = {uid: i for i, uid in enumerate(user_ids)}\n",
    "article_id_to_index = {aid: i for i, aid in enumerate(article_ids)}\n",
    "\n",
    "# Construction des listes 'row', 'col', 'data' pour la COO matrix\n",
    "row = [user_id_to_index[uid] for uid in df_user_item[\"user_id\"]]\n",
    "col = [article_id_to_index[aid] for aid in df_user_item[\"click_article_id\"]]\n",
    "data = df_user_item[\"click_count\"].values\n",
    "\n",
    "# Création de la matrice en format COO puis conversion en CSR\n",
    "sparse_user_item_matrix = coo_matrix(\n",
    "    (data, (row, col)),\n",
    "    shape=(len(user_ids), len(article_ids))\n",
    ").tocsr()\n",
    "\n",
    "# Vérification\n",
    "print(\"Taille de la matrice sparse (CSR) :\", sparse_user_item_matrix.shape)\n",
    "\n",
    "# Vérifier s'il manque des articles par rapport à df_clicks (normalement non).\n",
    "missing_from_matrix = set(unique_articles_in_clicks) - set(article_ids)\n",
    "print(f\"Nb d'articles présents dans df_clicks mais pas dans la matrice pivot: {len(missing_from_matrix)}\")\n",
    "if len(missing_from_matrix) > 0:\n",
    "    print(\"Exemple d'articles manquants:\", list(missing_from_matrix)[:20])\n",
    "    print(\"=> Possibles raisons : filtrage, ou IDs hors range, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du CF k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cf_model_knn(sparse_matrix, user_ids, article_ids):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle user-based CF avec NearestNeighbors sur la matrice sparse (CSR).\n",
    "    Retourne un dictionnaire contenant le modèle et les infos nécessaires.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - sparse_matrix : csr_matrix\n",
    "        Matrice (n_users x n_articles) contenant les interactions (count).\n",
    "    - user_ids : np.ndarray\n",
    "        Tableau des user_id uniques (indexés dans la matrice).\n",
    "    - article_ids : np.ndarray\n",
    "        Tableau des article_id uniques (indexés dans la matrice).\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - dict\n",
    "        Contient 'knn_model', 'user_ids', 'article_ids' etc.\n",
    "    \"\"\"\n",
    "    knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(sparse_matrix)\n",
    "\n",
    "    model_cf = {\n",
    "        'knn_model': knn_model,\n",
    "        'user_ids': user_ids,\n",
    "        'article_ids': article_ids,\n",
    "        'user_id_to_index': user_id_to_index,\n",
    "        'article_id_to_index': article_id_to_index,\n",
    "        'sparse_matrix': sparse_matrix\n",
    "    }\n",
    "    return model_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cf_knn(user_id, model_cf, sparse_matrix, top_n=5, k_neighbors=5):\n",
    "    \"\"\"\n",
    "    Recommande des articles à un utilisateur donné en utilisant le modèle k-NN user-based.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - user_id : int\n",
    "        Identifiant \"réel\" de l'utilisateur (pas l'indice interne)\n",
    "    - model_cf : dict\n",
    "        Dictionnaire contenant le modèle kNN et les tables de correspondance\n",
    "    - sparse_matrix : csr_matrix\n",
    "        Matrice user-item (n_users x n_articles)\n",
    "    - top_n : int\n",
    "        Nombre d'articles recommandés\n",
    "    - k_neighbors : int\n",
    "        Nombre de voisins les plus proches à agréger\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - list\n",
    "        Liste d'article_id recommandés (jusqu'à top_n).\n",
    "    \"\"\"\n",
    "    knn_model = model_cf['knn_model']\n",
    "    all_user_ids = model_cf['user_ids']\n",
    "    all_article_ids = model_cf['article_ids']\n",
    "    u2i = model_cf['user_id_to_index']\n",
    "\n",
    "    if user_id not in u2i:\n",
    "        # cold start => aucune reco\n",
    "        return []\n",
    "\n",
    "    # Récupérer l'index interne du user\n",
    "    user_idx = u2i[user_id]\n",
    "\n",
    "    # On récupère la ligne correspondant à cet utilisateur => shape (1, nb_articles) \n",
    "    user_vector = sparse_matrix[user_idx]\n",
    "\n",
    "    # Recherche des k voisins (on demande k+1 au cas où le user lui-même est dans les résultats)\n",
    "    distances, indices = knn_model.kneighbors(user_vector, n_neighbors=k_neighbors+1)\n",
    "\n",
    "    # indices[0] = [user_idx, neighbor1, neighbor2, ...]\n",
    "    neighbor_indices = indices[0].tolist()\n",
    "\n",
    "    # Exclure l'utilisateur lui-même si présent\n",
    "    if user_idx in neighbor_indices:\n",
    "        neighbor_indices.remove(user_idx)\n",
    "\n",
    "    # S'assurer d'avoir k voisins max\n",
    "    neighbor_indices = neighbor_indices[:k_neighbors]\n",
    "\n",
    "    # On récupère la sous-matrice pour ces voisins => shape (k_neighbors, nb_articles)\n",
    "    neighbors_matrix = sparse_matrix[neighbor_indices]\n",
    "\n",
    "    # On somme leurs usages d'article\n",
    "    article_scores = neighbors_matrix.sum(axis=0).A1  # .A1 => convertit en array 1D\n",
    "\n",
    "    # Exclure les articles déjà vus par l'utilisateur\n",
    "    already_viewed = user_vector.indices  # colonnes non nulles pour cet utilisateur\n",
    "    article_scores[already_viewed] = -9999  # on met un score très bas pour ignorer\n",
    "\n",
    "    # On récupère les top_n articles\n",
    "    top_article_indices = np.argsort(-article_scores)[:top_n]\n",
    "\n",
    "    # On convertit ces indices internes en article_id\n",
    "    recommended_articles = [all_article_ids[i] for i in top_article_indices]\n",
    "\n",
    "    return recommended_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle CF (k-NN)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntraînement du modèle CF (k-NN)...\")\n",
    "model_cf = fit_cf_model_knn(sparse_user_item_matrix, user_ids, article_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_profiles(articles_embeddings: np.ndarray,\n",
    "                        df_clicks: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Construit un profil d'utilisateur sous la forme d'un embedding moyen \n",
    "    des articles qu'il a consultés.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - articles_embeddings : np.ndarray\n",
    "        Matrice d'embeddings des articles (shape = (nb_articles, dims))\n",
    "        L'index i correspond à article_id = i (important !).\n",
    "    - df_clicks : pd.DataFrame\n",
    "        DataFrame des clics, contenant user_id et click_article_id.\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - dict\n",
    "        Clé : user_id\n",
    "        Valeur : np.ndarray (embedding moyen du user)\n",
    "    \"\"\"\n",
    "    user_profiles = {}\n",
    "    user_articles_map = df_clicks.groupby('user_id')['click_article_id'].apply(list)\n",
    "\n",
    "    for uid, articles_list in user_articles_map.items():\n",
    "        # Filtre si out-of-range\n",
    "        valid_articles = [a for a in articles_list if 0 <= a < len(articles_embeddings)]\n",
    "        if len(valid_articles) == 0:\n",
    "            continue\n",
    "        emb = articles_embeddings[valid_articles].mean(axis=0)\n",
    "        user_profiles[uid] = emb\n",
    "    return user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content_based(user_id: int,\n",
    "                            user_profiles: dict,\n",
    "                            articles_embeddings: np.ndarray,\n",
    "                            top_n: int = TOP_N) -> list:\n",
    "    \"\"\"\n",
    "    Recommande des articles pour un user_id donné en comparant son embedding moyen \n",
    "    à tous les articles via la similarité cosinus.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - user_id : int\n",
    "        Identifiant utilisateur\n",
    "    - user_profiles : dict\n",
    "        Clé : user_id, Valeur : embedding moyen (np.ndarray)\n",
    "    - articles_embeddings : np.ndarray\n",
    "        Matrice d'embeddings de tous les articles (shape = (nb_articles, dims))\n",
    "    - top_n : int\n",
    "        Nombre de recommandations à renvoyer\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - list\n",
    "        Liste des article_id recommandés (jusqu'à top_n).\n",
    "    \"\"\"\n",
    "    if user_id not in user_profiles:\n",
    "        # cold start => on ne peut pas faire de reco content-based\n",
    "        return []\n",
    "\n",
    "    user_emb = user_profiles[user_id].reshape(1, -1)  # shape(1, dims)\n",
    "\n",
    "    # Calcul de similarité cosinus avec tous les articles\n",
    "    sims = cosine_similarity(user_emb, articles_embeddings)[0]  # shape(nb_articles,)\n",
    "\n",
    "    # Tri descendant\n",
    "    sorted_indices = np.argsort(-sims)\n",
    "\n",
    "    # Top_n\n",
    "    return sorted_indices[:top_n].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Construction des profils utilisateurs pour le Content-Based...\n"
     ]
    }
   ],
   "source": [
    "# Construction des profils utilisateurs sur le df complet (pour la démo)\n",
    "print(\"\\nConstruction des profils utilisateurs pour le Content-Based...\")\n",
    "user_profiles_cb = build_user_profiles(articles_embeddings, df_clicks)\n",
    "model_cb = {\n",
    "    'user_profiles': user_profiles_cb,\n",
    "    'articles_embeddings': articles_embeddings\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde des modèles et des données transformées...\n",
      "Modèles et data_transform sauvegardés avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde de CF et CB dans des pickles\n",
    "print(\"Sauvegarde des modèles et des données transformées...\")\n",
    "\n",
    "with open(PATH_SAVE_CF_MODEL, 'wb') as f:\n",
    "    pickle.dump(model_cf, f)\n",
    "\n",
    "with open(PATH_SAVE_CB_MODEL, 'wb') as f:\n",
    "    pickle.dump(model_cb, f)\n",
    "\n",
    "# Sauvegarde des dimensions de la matrice (optionnel)\n",
    "data_transform = {\n",
    "    \"nb_users\": sparse_user_item_matrix.shape[0],\n",
    "    \"nb_articles\": sparse_user_item_matrix.shape[1]\n",
    "}\n",
    "\n",
    "with open(\"data_transform.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_transform, f)\n",
    "\n",
    "print(\"Modèles et data_transform sauvegardés avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un environnement Streamlit ou Azure Functions, on peut charger ces pickles \n",
    "et appeler les fonctions de prédiction ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(cf_model_path: str, cb_model_path: str):\n",
    "    \"\"\"\n",
    "    Charge les modèles CF et CB depuis des fichiers pickle.\n",
    "    \"\"\"\n",
    "    with open(cf_model_path, 'rb') as f:\n",
    "        model_cf_loaded = pickle.load(f)\n",
    "    with open(cb_model_path, 'rb') as f:\n",
    "        model_cb_loaded = pickle.load(f)\n",
    "    return model_cf_loaded, model_cb_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(user_id: int, \n",
    "                        model_cf: dict, \n",
    "                        sparse_matrix: csr_matrix,\n",
    "                        model_cb: dict, \n",
    "                        method: str = 'cf', \n",
    "                        top_n: int = TOP_N) -> list:\n",
    "    \"\"\"\n",
    "    Retourne une liste de recommandations pour un utilisateur donné, \n",
    "    selon la méthode choisie : 'cf' ou 'content'.\n",
    "\n",
    "    Paramètres:\n",
    "    -----------\n",
    "    - user_id : int\n",
    "        Identifiant utilisateur\n",
    "    - model_cf : dict\n",
    "        Modèle CF (k-NN) chargé\n",
    "    - sparse_matrix : csr_matrix\n",
    "        Matrice user-item\n",
    "    - model_cb : dict\n",
    "        Modèle Content-Based chargé\n",
    "    - method : str\n",
    "        'cf' ou 'content'\n",
    "    - top_n : int\n",
    "        Nombre de recommandations\n",
    "\n",
    "    Retour:\n",
    "    -------\n",
    "    - list\n",
    "        Liste d'articles recommandés\n",
    "    \"\"\"\n",
    "    if method == 'cf':\n",
    "        return predict_cf_knn(user_id, model_cf, sparse_matrix, top_n=top_n)\n",
    "    elif method == 'content':\n",
    "        user_profiles_cb = model_cb['user_profiles']\n",
    "        articles_embeddings_cb = model_cb['articles_embeddings']\n",
    "        return recommend_content_based(user_id, user_profiles_cb, articles_embeddings_cb, top_n=top_n)\n",
    "    else:\n",
    "        raise ValueError(\"Méthode inconnue. Choisir 'cf' ou 'content'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple d'appel\n",
    "(Dans la pratique, on chargerait d'abord les modèles, puis on exécuterait get_recommendations() pour chaque requête.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommandations CF pour l'utilisateur 12345 : [np.int64(96210), np.int64(284985), np.int64(194920), np.int64(5278), np.int64(236566)]\n",
      "Recommandations CB pour l'utilisateur 12345 : [111862, 95246, 231146, 133356, 84624]\n"
     ]
    }
   ],
   "source": [
    "example_user_id = 12345\n",
    "cf_recos = get_recommendations(example_user_id, model_cf, csr_matrix(sparse_user_item_matrix), model_cb, method='cf', top_n=TOP_N)\n",
    "cb_recos = get_recommendations(example_user_id, model_cf, csr_matrix(sparse_user_item_matrix), model_cb, method='content', top_n=TOP_N)\n",
    "\n",
    "print(f\"Recommandations CF pour l'utilisateur {example_user_id} : {cf_recos}\")\n",
    "print(f\"Recommandations CB pour l'utilisateur {example_user_id} : {cb_recos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_projet_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
